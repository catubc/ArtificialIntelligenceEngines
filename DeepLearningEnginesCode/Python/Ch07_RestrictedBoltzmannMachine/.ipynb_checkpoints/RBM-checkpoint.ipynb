{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c76ccaf4-6711-48c8-a808-55535d29372b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(120000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 120 seconds\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "RBM network.\n",
    "@author: Gabriel Bianconi, modified by JimStone\n",
    "# This takes about 10 minutes to run on a 2015 mac. \n",
    "# Description: Applies a single 2-layer RBM to the MNIST dataset of images of digits from 0-9. The trained model uses a SciPy-based logistic regression to classify outputs. It achieves 92.8% classification accuracy on the test set of images.\n",
    "Result: 9237/10000\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import torch\n",
    "import torchvision.datasets\n",
    "import torchvision.models\n",
    "import torchvision.transforms\n",
    "\n",
    "%autosave 120\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e87e5db-73a7-402d-b5ef-f5fe2b793eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "###################################################\n",
    "###################################################\n",
    "\n",
    "BATCH_SIZE = 64 # batch size for training\n",
    "\n",
    "VISIBLE_UNITS = 784  # 784 = 28 x 28 images\n",
    "\n",
    "HIDDEN_UNITS = 128\n",
    "\n",
    "CD_K = 2 # number of contrastive divergence (CD) steps per training vector\n",
    "\n",
    "EPOCHS = 2 # number of times the entire data set is used to train RBM\n",
    "\n",
    "DATA_FOLDER = '/home/cat/data/mnist'\n",
    "\n",
    "# CUDA = Compute Unified Device Architecture, a GPU processor.\n",
    "# Don't worry if you don't have this, the code can work without it.\n",
    "CUDA = torch.cuda.is_available()\n",
    "CUDA_DEVICE = 0\n",
    "\n",
    "CUDA = False\n",
    "\n",
    "if CUDA:\n",
    "#if False:\n",
    "    torch.cuda.set_device(CUDA_DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc1bebf6-630c-474d-a7c6-fb22ef845220",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "###################################################\n",
    "###################################################\n",
    "\n",
    "class RBM():\n",
    "\n",
    "    def __init__(self, \n",
    "                 num_visible,                     \n",
    "                 num_hidden, \n",
    "                 k, \n",
    "                 learning_rate=1e-3, \n",
    "                 momentum_coefficient=0.5,\n",
    "                 weight_decay=1e-4, \n",
    "                 use_cuda=True):\n",
    "        \n",
    "        #\n",
    "        self.num_visible    = num_visible               # no. of visible units; should match the input\n",
    "                                                        #   data dimensions\n",
    "        \n",
    "        #\n",
    "        self.num_hidden     = num_hidden                # no. of hidden units; usually less than input to \n",
    "                                                        #   foster compression/denoising like behavior\n",
    "            \n",
    "        #\n",
    "        self.k              = k                         # number of contrastive divergence steps\n",
    "        \n",
    "        #\n",
    "        self.learning_rate  = learning_rate             # learning rate\n",
    "        \n",
    "        # \n",
    "        self.momentum_coefficient = momentum_coefficient   # momentum; description\n",
    "        \n",
    "        #\n",
    "        self.weight_decay   = weight_decay              # rate of decay of learning rate? \n",
    "        \n",
    "        #\n",
    "        self.use_cuda       = use_cuda                  # cuda flag\n",
    "\n",
    "        # weights between input and hidden units\n",
    "        self.weights        = torch.randn(num_visible, num_hidden) * 0.1    # initiailization of weights between input and hidden units\n",
    "                                                                            #  - the sampling comes from \"standard normal distribution, \n",
    "                                                                            #               \"that is mean is 0 and variance is 1.\"\n",
    "        \n",
    "        # bias terms of visible units\n",
    "        self.visible_bias   = torch.ones(num_visible) * 0.5                 # bias units for the visible units set to 0.5 for all nodes\n",
    "        \n",
    "        # bias terms of hidden units\n",
    "        self.hidden_bias    = torch.zeros(num_hidden)                       # hidden units have 0 valued bias units (not sure if they change later?!)\n",
    "\n",
    "        # create space for momentum parameters\n",
    "        self.weights_momentum       = torch.zeros(num_visible, num_hidden)  # individual edges betweeen units have their own momentum parameters? \n",
    "                                                                            #   starts at zero\n",
    "        #\n",
    "        self.visible_bias_momentum  = torch.zeros(num_visible)              # visible units also have own momentum, initailized to zero\n",
    "        \n",
    "        #\n",
    "        self.hidden_bias_momentum   = torch.zeros(num_hidden)               # hidden units also have own momentum, initialized to zero\n",
    "\n",
    "        # convert all parameters to cuda variables if GPU is avialble \n",
    "        if self.use_cuda:\n",
    "            self.weights        = self.weights.cuda()\n",
    "            self.visible_bias   = self.visible_bias.cuda()\n",
    "            self.hidden_bias    = self.hidden_bias.cuda()\n",
    "\n",
    "            self.weights_momentum       = self.weights_momentum.cuda()\n",
    "            self.visible_bias_momentum  = self.visible_bias_momentum.cuda()\n",
    "            self.hidden_bias_momentum   = self.hidden_bias_momentum.cuda()\n",
    "    \n",
    "    #\n",
    "    def sample_hidden(self, visible_probabilities):\n",
    "        \n",
    "        ''' Sample hidden function\n",
    "            1. Computes the total input into the hidden layer\n",
    "                - so takes the input data (aka \"visible_probabilities\") of size 748 each (64 samples depending on batch size)\n",
    "                - and computes the sum of the product of each input sample with the weigths for a specific unit \n",
    "                - this sum then represents the observed input by each hidden layer unit (based on current input and weights)\n",
    "            \n",
    "            2. Then we apply a raw (i.e. standard parameter) sigmoid activation to the total input; \n",
    "            -  the sigmoid is the activation function of the hidden units;\n",
    "            -  it projects the input always onto a range of 0 to 1\n",
    "            -  the part of the input data that falls between ~-2 to ~+2 is somewhat linearly mapped \n",
    "            \n",
    "            3. The output is hidden_probabilies, essentially representing the states of the hidden-layer nodes\n",
    "            -  not completely clear why they are called \"probabilities\" instead of states;\n",
    "            -  perhaps clearer below, but might have to do with Boltzman Machine terminiology\n",
    "            \n",
    "            Note: hidden_bias is set to zero initially (to check if changes over time?)\n",
    "            \n",
    "            Input:  a batch of data, e.g. 64 (samples) x 748 (size of each sample)\n",
    "            Output: hidden unit representations of the batch data, e.g. 64 (samples) x 128 (size of each sample in the hidden layer)\n",
    "            \n",
    "        '''\n",
    "        \n",
    "        # get input to each hidden unit\n",
    "        hidden_activations = torch.matmul(visible_probabilities, self.weights) + self.hidden_bias\n",
    "        \n",
    "        # use hidden unit inputs to find prob(on)=hidden_probabilities\n",
    "        hidden_probabilities = self._sigmoid(hidden_activations)\n",
    "        \n",
    "        #\n",
    "        return hidden_probabilities\n",
    "    \n",
    "    #\n",
    "    def sample_visible(self, hidden_probabilities):\n",
    "        \n",
    "        # get input to each visible unit\n",
    "        visible_activations = torch.matmul(hidden_probabilities, self.weights.t()) + self.visible_bias\n",
    "        \n",
    "        # use visible unit inputs to find prob(on)=hidden_probabilities\n",
    "        visible_probabilities = self._sigmoid(visible_activations)\n",
    "        \n",
    "        #\n",
    "        return visible_probabilities\n",
    "    \n",
    "    #\n",
    "    def _sigmoid(self, x):\n",
    "        \n",
    "        ''' Applies sigmoid function to input data\n",
    "            \n",
    "            Note: sigmoid transform sets most values ~< -4 to 0 and those ~> +4 to 1; \n",
    "                - values in between range from 0 to 1\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        #\n",
    "        return 1 / (1 + torch.exp(-x))\n",
    "\n",
    "    #\n",
    "    def _random_probabilities(self, num):\n",
    "        \n",
    "        #\n",
    "        random_probabilities = torch.rand(num)\n",
    "\n",
    "        #\n",
    "        if self.use_cuda:\n",
    "            random_probabilities = random_probabilities.cuda()\n",
    "        \n",
    "        #\n",
    "        return random_probabilities\n",
    "    \n",
    "    # ===========================================================\n",
    "    def contrastive_divergence(self, input_data):\n",
    "        \n",
    "        # input_data is 64 (batch size) by 784 (real valued pixels in input image)\n",
    "        # =Positive phase==================================================\n",
    "        # Positive phase = use 'clamped' visible unit states to sample hidden unit states.\n",
    "        # sample_hidden() treats each real-valued pixel as a probability\n",
    "        # \n",
    "        # SEE FUNCTION DESCRIPTION ABOVE\n",
    "        positive_hidden_probabilities  = self.sample_hidden(input_data) # 64 x 128 hidden units\n",
    "    \n",
    "        # use positive_hidden_probabilities to get sample of binary hidden unit states\n",
    "        positive_hidden_activations = (positive_hidden_probabilities >= self._random_probabilities(self.num_hidden)).float() # BATCH_SIZE = 64 x 128 hidden units\n",
    "        \n",
    "        positive_associations = torch.matmul(input_data.t(), positive_hidden_activations)\n",
    "        # print((positive_associations.shape)) # torch.Size([784, 128]) HIDDEN_UNITS = 128\n",
    "        # .t() = transpose: https://pytorch.org/docs/0.3.1/torch.html#torch.t\n",
    "        # positive_associations measures correlation between visible and hidden unit states when visible units are  clamped to training data.\n",
    "        \n",
    "        # =Negative phase==================================================\n",
    "        # Negative phase, initialise with final binary positive_hidden_activations\n",
    "        hidden_activations = positive_hidden_activations # 64 x 128\n",
    "        \n",
    "        # loop over number of CD steps\n",
    "        for step in range(self.k): # number of contrastive divergence steps\n",
    "            visible_probabilities   = self.sample_visible(hidden_activations)\n",
    "            hidden_probabilities    = self.sample_hidden(visible_probabilities)\n",
    "            hidden_activations      = (hidden_probabilities >= self._random_probabilities(self.num_hidden)).float()\n",
    "\n",
    "        negative_visible_probabilities  = visible_probabilities\n",
    "        negative_hidden_probabilities   = hidden_probabilities\n",
    "        \n",
    "        # negative_associations measures correlation between visible and hidden unit states when visible units are not clamped to training data.\n",
    "        negative_associations = torch.matmul(negative_visible_probabilities.t(), negative_hidden_probabilities)\n",
    "\n",
    "        # Update weight change\n",
    "        self.weights_momentum *= self.momentum_coefficient\n",
    "        self.weights_momentum += (positive_associations - negative_associations)\n",
    "\n",
    "        # Update visible bias terms\n",
    "        self.visible_bias_momentum *= self.momentum_coefficient\n",
    "        self.visible_bias_momentum += torch.sum(input_data - negative_visible_probabilities, dim=0)\n",
    "\n",
    "        # Update hidden bias terms\n",
    "        self.hidden_bias_momentum *= self.momentum_coefficient\n",
    "        self.hidden_bias_momentum += torch.sum(positive_hidden_probabilities - negative_hidden_probabilities, dim=0)\n",
    "\n",
    "        batch_size = input_data.size(0)\n",
    "\n",
    "        self.weights        += self.weights_momentum * self.learning_rate / batch_size\n",
    "        self.visible_bias   += self.visible_bias_momentum * self.learning_rate / batch_size\n",
    "        self.hidden_bias    += self.hidden_bias_momentum * self.learning_rate / batch_size\n",
    "\n",
    "        self.weights -= self.weights * self.weight_decay  # L2 weight decay\n",
    "\n",
    "        # Compute reconstruction error\n",
    "        error = torch.sum((input_data - negative_visible_probabilities)**2)\n",
    "\n",
    "        return error\n",
    "# ===========================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2db62cdf-e97b-4c61-b7bf-02033f4d2b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MNIST dataset of images of digits between 0 and 9 ...\n"
     ]
    }
   ],
   "source": [
    "###################################################\n",
    "###################################################\n",
    "###################################################\n",
    "\n",
    "#\n",
    "print('Loading MNIST dataset of images of digits between 0 and 9 ...')\n",
    "\n",
    "# training data\n",
    "train_dataset = torchvision.datasets.MNIST(root=DATA_FOLDER, \n",
    "                                           train=True, \n",
    "                                           transform=torchvision.transforms.ToTensor(), \n",
    "                                           download=True)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                           batch_size=BATCH_SIZE)\n",
    "\n",
    "# test data; download just the test dataset\n",
    "test_dataset = torchvision.datasets.MNIST(root=DATA_FOLDER, \n",
    "                                          train=False, \n",
    "                                          transform=torchvision.transforms.ToTensor(), \n",
    "                                          download=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, \n",
    "                                          batch_size=BATCH_SIZE)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47333c0e-a289-4312-bc41-53f3653239b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RBM for 2 epochs ...\n",
      "epoch:  0   batch:  0 torch.Size([64, 784])\n",
      "epoch:  0   batch:  100 torch.Size([64, 784])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16980/99753948.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mbatch_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrastive_divergence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_16980/1969229144.py\u001b[0m in \u001b[0;36mcontrastive_divergence\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mpositive_hidden_activations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpositive_hidden_probabilities\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_random_probabilities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# BATCH_SIZE = 64 x 128 hidden units\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mpositive_associations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositive_hidden_activations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;31m# print((positive_associations.shape)) # torch.Size([784, 128]) HIDDEN_UNITS = 128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;31m# .t() = transpose: https://pytorch.org/docs/0.3.1/torch.html#torch.t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "###################################################\n",
    "###################################################\n",
    "###################################################\n",
    "\n",
    "print('Training RBM for %d epochs ...' % EPOCHS)\n",
    "# create RBM network with one visible and one hidden laayer\n",
    "rbm = RBM(VISIBLE_UNITS,   # total number of units in the input layer; Q: is this the same for output layer also? \n",
    "                           #   A: likely this RBM version only has input and hidden layers\n",
    "          HIDDEN_UNITS, \n",
    "          CD_K, \n",
    "          use_cuda=CUDA)\n",
    "\n",
    "#\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_error = 0.0\n",
    "\n",
    "    ctr = 0\n",
    "    # the train loader has split the 60,000 samples into batches of 64 each; so in total, there are about 937/938 (presumably unique) batches of data\n",
    "    #  each data trial is 28 x 28 in size, so 784\n",
    "    for batch, _ in train_loader:\n",
    "        batch = batch.view(len(batch), VISIBLE_UNITS)  # flatten input data\n",
    "\n",
    "        if CUDA:\n",
    "            batch = batch.cuda()\n",
    "        \n",
    "        if ctr%100==0:\n",
    "            print (\"epoch: \", epoch, \"  batch: \", ctr, batch.shape)\n",
    "        #print (batch)\n",
    "        \n",
    "        #\n",
    "        batch_error = rbm.contrastive_divergence(batch)\n",
    "\n",
    "        #\n",
    "        epoch_error += batch_error\n",
    "        ctr+=1\n",
    "\n",
    "    print('Epoch Error (epoch=%d): %.4f' % (epoch, epoch_error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b04fd2f-22ca-462c-887d-134aa5d6d35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features...\n",
      "testing train loader...\n",
      "testing testloader...\n",
      "DONE...\n"
     ]
    }
   ],
   "source": [
    "###################################################\n",
    "###################################################\n",
    "###################################################\n",
    "print('Extracting features...')\n",
    "\n",
    "train_features = np.zeros((len(train_dataset), HIDDEN_UNITS))\n",
    "train_labels = np.zeros(len(train_dataset))\n",
    "test_features = np.zeros((len(test_dataset), HIDDEN_UNITS))\n",
    "test_labels = np.zeros(len(test_dataset))\n",
    "\n",
    "# \n",
    "print (\"testing train loader...\")\n",
    "for i, (batch, labels) in enumerate(train_loader):\n",
    "    batch = batch.view(len(batch), VISIBLE_UNITS)  # flatten input data\n",
    "\n",
    "    if CUDA:\n",
    "        batch = batch.cuda()\n",
    "\n",
    "    train_features[i*BATCH_SIZE:i*BATCH_SIZE+len(batch)] = rbm.sample_hidden(batch).cpu().numpy()\n",
    "    train_labels[i*BATCH_SIZE:i*BATCH_SIZE+len(batch)] = labels.numpy()\n",
    "\n",
    "#\n",
    "print (\"testing testloader...\")\n",
    "for i, (batch, labels) in enumerate(test_loader):\n",
    "    batch = batch.view(len(batch), VISIBLE_UNITS)  # flatten input data\n",
    "\n",
    "    if CUDA:\n",
    "        batch = batch.cuda()\n",
    "\n",
    "    test_features[i*BATCH_SIZE:i*BATCH_SIZE+len(batch)] = rbm.sample_hidden(batch).cpu().numpy()\n",
    "    test_labels[i*BATCH_SIZE:i*BATCH_SIZE+len(batch)] = labels.numpy()\n",
    "\n",
    "print (\"DONE...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1407b7d9-126d-4538-a779-bcc97e96ab9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting linear classifier ...\n",
      "Result: 9271/10000\n"
     ]
    }
   ],
   "source": [
    "###################################################\n",
    "###################################################\n",
    "###################################################\n",
    "\n",
    "# This last step \n",
    "\n",
    "print('Fitting linear classifier ...')\n",
    "\n",
    "clf = LogisticRegression(solver='newton-cg',multi_class='auto')\n",
    "\n",
    "# train_features = 60,000 hidden unit outputs (1 per training vector) x 128 hidden units\n",
    "# use outputs of trained hidden units to fit linear classifier to correct labels ...\n",
    "clf.fit(train_features, train_labels)\n",
    "\n",
    "# use fitted linear classifier to classify test data ... test_features = 10,000 hidden layer states, 1 per test vector\n",
    "predictions = clf.predict(test_features) # predictions = 10,000 classifications (0-9)\n",
    "\n",
    "# compare 10,000 classification results on test data with correct class labels ...\n",
    "print('Result: %d/%d' % (sum(predictions == test_labels), test_labels.shape[0]))\n",
    "# prints out (for example, on test data) Result: 9244/10000 = 92.44%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be5e576-75e7-4215-9574-fed2a9efbd07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
